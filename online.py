# -*- coding: utf-8 -*-
"""online.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1G1OWINnmXgJ7FZ2F5zo736vj2Rau07_H

Not finished yet :/
"""

import pickle

# Load the serialized model from disk
with open("best_model.pkl", "rb") as file:
    model = pickle.load(file)

pip install pyspark

# Import SparkSession
from pyspark.sql import SparkSession
# Create a Spark Session
spark = SparkSession.builder.master("local[*]").getOrCreate()

from pyspark.sql.streaming import DataStreamReader

# Read the data from the Kafka topic as a streaming dataframe
df = spark.readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "host1:port1,host2:port2")
  .option("subscribe", "topic_name")
  .load()

# Define the custom function
def process_batch(df, batch_id):
    # Your custom processing logic here
    print("Processing batch: ", batch_id)
    predicted = model.predict(df)

    spark_df = spark.createDataFrame(predicted)

    df.write
      .format("kafka")
      .option("kafka.bootstrap.servers", "host1:port1,host2:port2")
      .option("topic", "predicted_results")
      .save()

# Start the streaming query with a trigger to process the new data every 10 seconds
query = df
  .writeStream
  .foreachBatch(process_batch)
  .trigger(processingTime="10 seconds")
  .start()

query.awaitTermination()