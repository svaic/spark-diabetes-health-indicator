# -*- coding: utf-8 -*-
"""online.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1G1OWINnmXgJ7FZ2F5zo736vj2Rau07_H

Not finished yet :/
"""

import pickle

# Load the serialized model from disk
model = None

with open("best_model.pkl", "rb") as file:
    model = pickle.load(file)

pip install pyspark

# Import SparkSession
from pyspark.sql import SparkSession
# Create a Spark Session
spark = SparkSession.builder.master("local[*]").getOrCreate()

from pyspark.sql.streaming import DataStreamReader

# Read the data from the Kafka topic as a streaming dataframe
df = spark.readStream\
  .format("kafka")\
  .option("kafka.bootstrap.servers", "host1:port1,host2:port2")\
  .option("subscribe", "topic_name")\
  .load()

def get_age_category(number):
  if number < 4:
    return 0
  if number >= 4 and number <= 8:
    return 1
  if number >= 8 and number <= 12:
    return 2
  if number >= 12:
    return 3

age_category_function = udf(get_age_category, IntegerType())

# Define the custom function
def process_batch(df, batch_id):
    # Your custom processing logic here
    print("Processing batch: ", batch_id)

    df = df.withColumn("age_category", age_category_function(col("Age")))

    predicted = model.predict(df.toPandas())

    spark_df = spark.createDataFrame(predicted)

    spark_df.write\
      .format("kafka")\
      .option("kafka.bootstrap.servers", "host1:port1,host2:port2")\
      .option("topic", "predicted_results")\
      .save()

# Start the streaming query with a trigger to process the new data every 10 seconds
query = df\
  .writeStream\
  .foreachBatch(process_batch)\
  .trigger(processingTime="10 seconds")\
  .start()

query.awaitTermination()