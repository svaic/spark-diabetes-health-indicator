# -*- coding: utf-8 -*-
"""Rudarenje_d3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15QwvgUYWuZ4-t89npRjP4Ph_tWsaYIgl
"""

import pandas as pd
from sklearn.model_selection import train_test_split

# Load the CSV file into a pandas dataframe
df = pd.read_csv("db.csv")

df.head()

# Split the data into features (X) and labels (y)
X = df.drop("Diabetes_binary", axis=1)
y = df["Diabetes_binary"]

# Split the data into train and test datasets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

X_train.to_csv('x_train.csv')
y_train.to_csv('y_train.csv')
X_test.to_csv('x_test.csv')
y_test.to_csv('y_test.csv')

pip install pyspark

# Import SparkSession
from pyspark.sql import SparkSession
# Create a Spark Session
spark = SparkSession.builder.master("local[*]").getOrCreate()

df = spark.read.csv("x_train.csv", header=True, inferSchema=True)

pandas_df = df.toPandas()

import matplotlib.pyplot as plt

plt.hist(pandas_df["Age"], bins=50)
plt.xlabel("Age")
plt.ylabel("Frequency")
plt.title("Histogram of Age")
plt.show()

plt.hist(pandas_df["Sex"], bins=3)
plt.xlabel("Sex")
plt.ylabel("Frequency")
plt.title("Histogram of Age")
plt.show()

plt.plot(pandas_df["MentHlth"].value_counts().sort_index())
plt.xlabel("MentHlth")
plt.ylabel("Frequency")
plt.title("Histogram of MentHlth")
plt.show()

import matplotlib.pyplot as plt

plt.hist(pandas_df["BMI"], bins=50)
plt.xlabel("BMI")
plt.ylabel("Frequency")
plt.title("Histogram of Age")
plt.show()

plt.hist(pandas_df["Education"], bins=50)
plt.xlabel("Education")
plt.ylabel("Frequency")
plt.title("Histogram of Age")
plt.show()

from pyspark.sql.functions import udf, col
from pyspark.sql.types import IntegerType

def get_age_category(number):
  if number < 4:
    return 0
  if number >= 4 and number <= 8:
    return 1
  if number >= 8 and number <= 12:
    return 2
  if number >= 12:
    return 3

age_category_function = udf(get_age_category, IntegerType())

df = df.withColumn("age_category", age_category_function(col("Age")))

df.show(5)

df_sample = df.select("age", "HighBP", "HvyAlcoholConsump", "PhysHlth", "Sex", "GenHlth", "HeartDiseaseorAttack")

df_sample = df

df_sample.head()

X_train_offline, X_test_offline, y_train_offline, y_test_offline = train_test_split(df_sample.toPandas(), y_train, test_size=0.2, random_state=0)

import pandas as pd 

def train_model(model):
  clf = model # defining decision tree classifier
  clf=clf.fit(X_train_offline, y_train_offline) # train data on new data and new target
  y_pred = pd.Series(clf.predict(X_test_offline)) #  assign removed data as input

  f1_score = metrics.f1_score(y_pred, y_test_offline)
  return f1_score, clf

from sklearn.linear_model import LogisticRegression

train_model(LogisticRegression())

from sklearn import tree

train_model(tree.DecisionTreeClassifier())

from sklearn.ensemble import RandomForestClassifier

train_model(RandomForestClassifier())

from sklearn.model_selection import GridSearchCV
from sklearn.metrics import make_scorer, f1_score
from sklearn import tree

# Define the model and parameters to be searched
model = tree.DecisionTreeClassifier()
param_grid = {'max_depth': [20, 30, 50, 100], 'max_features': [5, 10, 15, 20], 'criterion': ['entropy']}

# Create the grid search object
grid_search = GridSearchCV(model, param_grid, cv=5, scoring=make_scorer(f1_score))

# Fit the grid search to the data
grid_search.fit(X_train_offline, y_train_offline)

# Get the best hyperparameters
best_params = grid_search.best_params_

grid_search.best_score_

from sklearn.model_selection import GridSearchCV
from sklearn.metrics import make_scorer, f1_score


def train_model_and_search_hyper_param(model, param_grid):
  # Create the grid search object
  grid_search = GridSearchCV(model, param_grid, cv=5, scoring=make_scorer(f1_score))

  # Fit the grid search to the data
  grid_search.fit(X_train_offline, y_train_offline)

  # Get the best hyperparameters
  best_params = grid_search.best_params_

  y_pred = pd.Series(grid_search.best_estimator_.predict(X_test_offline)) #  assign removed data as input

  f1_score_rez = metrics.f1_score(y_pred, y_test_offline)

  print(best_params)
  print(f1_score_rez)

from sklearn import tree

model = tree.DecisionTreeClassifier()
param_grid = {'max_depth': [20, 30, 50, 100], 'max_features': [5, 10, 15, 20], 'criterion': ['entropy']}

train_model_and_search_hyper_param(model, param_grid)

from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier()
param_grid = {'n_estimators': [5, 10, 25], 'max_depth': [20, 30, 50, 100], 'max_features': [5, 10, 15, 20], 'criterion': ['entropy']}

train_model_and_search_hyper_param(model, param_grid)

from sklearn.linear_model import LogisticRegression

model = LogisticRegression()
param_grid = {'C': [0.01, 10, 100], 'penalty': ["l1", "l2"], 'max_iter': [10,20]}

train_model_and_search_hyper_param(model, param_grid)

from sklearn import tree

score, model = train_model(tree.DecisionTreeClassifier(criterion='entropy', max_depth=100, max_features=10))

score

import pickle

# Serialize the model to disk
with open("best_model.pkl", "wb") as file:
    pickle.dump(model, file)